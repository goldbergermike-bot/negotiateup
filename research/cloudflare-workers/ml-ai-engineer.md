---
company: cloudflare-workers
company_display: Cloudflare Workers
role: ml-ai-engineer
role_display: ML/AI Engineer
role_type: standard
last_updated: 2026-02-23
data_quality: high
salary_data_quarter: 2025-Q4
next_review_due: 2026-05-23
compensation:
  - region: San Francisco
    base_low: 198000
    base_high: 255000
    stock_low: 215000
    stock_high: 378000
    bonus_pct: 10
    total_comp_low: 310000
    total_comp_high: 432000
    currency: USD
  - region: Austin
    base_low: 188000
    base_high: 245000
    stock_low: 205000
    stock_high: 362000
    bonus_pct: 10
    total_comp_low: 298000
    total_comp_high: 418000
    currency: USD
  - region: London
    base_low: 132000
    base_high: 170000
    stock_low: 142000
    stock_high: 250000
    bonus_pct: 10
    total_comp_low: 205000
    total_comp_high: 288000
    currency: GBP
level_mapping:
  internal: null
  raw: Cloudflare ML/AI (L3-L5) = Google ML L4-L6 = Meta ML E4-E6 = Fastly AI = AWS Inferentia ML
data_sources:
  - Levels.fyi
negotiation_dna_summary: "RSU-Heavy (NYSE: NET) + Bonus | Edge AI Inference & Model Optimization | $30B+ Market Cap | 20M+ Developers | **WORKERS AI PREMIUM**"
---
### ML/AI Engineer | Cloudflare Workers Global Negotiation Guide

**Negotiation DNA:** RSU-Heavy (NYSE: NET) + Bonus | Edge AI Inference & Model Optimization | $30B+ Market Cap | 20M+ Developers | **WORKERS AI PREMIUM**

| Region | Base Salary | Stock (RSU/4yr) | Bonus | Total Comp |
|--------|-------------|-----------------|-------|------------|
| San Francisco | $198Kâ€“$255K | $215Kâ€“$378K | 10â€“15% | $310Kâ€“$432K |
| Austin | $188Kâ€“$245K | $205Kâ€“$362K | 10â€“15% | $298Kâ€“$418K |
| London | Â£132Kâ€“Â£170K | Â£142Kâ€“Â£250K | 10â€“15% | Â£205Kâ€“Â£288K |

**Negotiation DNA**

Cloudflare Workers ML/AI Engineers build the AI inference platform running at the edge â€” optimizing models for deployment across 300+ data centers, building the inference serving infrastructure for Workers AI, and developing the AI Gateway that enables developers to manage AI API calls at the edge. In February 2026, this is Cloudflare's fastest-growing engineering function, as Workers AI transforms from experiment to production platform serving millions of AI inference requests daily.

As a $30B+ NYSE company investing heavily in AI infrastructure, Cloudflare offers competitive ML/AI compensation with liquid RSUs. The edge AI inference challenge is unique â€” models must run efficiently on heterogeneous hardware across globally distributed locations with strict latency constraints.

**Level Mapping:** Cloudflare ML/AI (L3-L5) = Google ML L4-L6 = Meta ML E4-E6 = Fastly AI = AWS Inferentia ML

### ðŸ—ï¸ Cloudflare Workers AI Edge Inference Lever

Cloudflare's 2026 Workers AI strategy requires ML/AI Engineers to optimize AI model inference for edge deployment â€” quantizing models for efficient execution on edge GPUs, building inference serving systems that handle variable hardware across 300+ locations, and developing the AI Gateway that provides developers with unified access to multiple AI providers at the edge.

ML/AI Engineers must solve unique optimization challenges: models must perform well on diverse GPU hardware, inference must complete within edge-appropriate latencies, and the system must support both open-source models and gateway routing to external AI providers.

**Global Levers**

1. **Edge AI Inference Optimization:** "I optimize AI model inference for Cloudflare's global edge â€” quantizing and deploying models across 300+ data centers with heterogeneous GPU hardware. Edge-optimized AI inference at this geographic scale is a frontier ML challenge."
2. **Workers AI Model Serving:** "I build the inference serving infrastructure for Workers AI â€” handling millions of daily AI requests with variable hardware, strict latency requirements, and automatic scaling across the global edge network."
3. **AI Gateway Platform:** "I develop Cloudflare's AI Gateway â€” enabling developers to manage, cache, and rate-limit AI API calls at the edge. Building an AI API management layer at planetary scale requires deep understanding of both ML systems and distributed computing."
4. **AWS Inferentia/Google TPU/Meta Competition:** "AWS Inferentia, Google TPU, and Meta are competing for ML engineers who can optimize AI inference at infrastructure scale. Cloudflare must offer competitive comp to retain ML talent with edge AI optimization expertise."

> **Negotiate Up Strategy:** "I'm targeting $248K base and $368K RSUs over 4 years with 15% performance bonus for this ML/AI Engineer role. I optimize AI inference for Cloudflare's global edge â€” deploying models across 300+ data centers for 20M+ developers. ML engineers with edge AI inference optimization expertise are in extreme shortage. I have competing offers from [Google ML at $420K TC / AWS ML at $405K TC / Meta ML at $425K TC]. ML engineers who can optimize AI inference for globally distributed edge hardware are the scarcest AI talent in 2026." Accept at $220K+ base and $305K+ RSUs.

**Evidence & Sources**
- [Cloudflare Workers AI â€” 2026 Edge Inference Platform]
- [Cloudflare ML/AI Engineer Comp â€” Levels.fyi 2025-2026]
- [Cloudflare $30B+ Market Cap â€” NYSE: NET]
- [Edge AI Inference â€” AWS/Google/Meta Competition 2026]
