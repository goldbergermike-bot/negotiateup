---
company: anthropic
company_display: Anthropic
role: constitutional-ai-research-engineer
role_display: Constitutional AI Research Engineer
role_type: specialty
last_updated: 2026-02-23
data_quality: high
salary_data_quarter: 2025-Q4
next_review_due: 2026-05-23
compensation:
  - region: San Francisco
    base_low: 295000
    base_high: 370000
    stock_low: 500000
    stock_high: 875000
    bonus_pct: null
    total_comp_low: 420000
    total_comp_high: 589000
    currency: USD
  - region: Seattle
    base_low: 286000
    base_high: 352000
    stock_low: 500000
    stock_high: 875000
    bonus_pct: null
    total_comp_low: 411000
    total_comp_high: 571000
    currency: USD
  - region: London
    base_low: 224000
    base_high: 281000
    stock_low: 380000
    stock_high: 665000
    bonus_pct: null
    total_comp_low: 319000
    total_comp_high: 448000
    currency: GBP
level_mapping:
  internal: null
  raw: Anthropic Constitutional AI Research Engineer = Google L6+ Research Scientist = DeepMind Senior Research Scientist = OpenAI Research Engineer = Meta FAIR Research Scientist IC6+
data_sources:
  - Levels.fyi
  - Blind
negotiation_dna_summary: "Base-Heavy ($300K+) + Pre-IPO Equity | AI Safety Pioneer | Safety Premium | **SIGNATURE ROLE** | **+20-35% AGENTIC AI PREMIUM**"
---
### Constitutional AI Research Engineer | Anthropic Global Negotiation Guide

**Negotiation DNA:** Base-Heavy ($300K+) + Pre-IPO Equity | AI Safety Pioneer | Safety Premium | **SIGNATURE ROLE** | **+20-35% AGENTIC AI PREMIUM**

| Region | Base Salary | Equity (Pre-IPO/4yr) | Bonus | Total Comp |
|--------|-------------|---------------------|-------|------------|
| San Francisco | $295K-$370K | $500K-$875K | — | $420K-$589K |
| Seattle | $286K-$352K | $500K-$875K | — | $411K-$571K |
| London | £224K-£281K | £380K-£665K | — | £319K-£448K |

**Negotiation DNA**
The Constitutional AI Research Engineer is Anthropic's signature role — the position that defines what makes Anthropic different from every other AI company on earth. You will build the alignment and safety systems that make Claude safe, helpful, and honest: Constitutional AI training pipelines, RLHF reward models, interpretability tools, red-teaming frameworks, and the evaluation infrastructure that determines whether a model is safe enough to deploy. This is the role Dario and Daniela Amodei founded Anthropic to create — the intersection of world-class machine learning engineering and rigorous AI safety research. Constitutional AI Research Engineers don't just build AI; they build the guardrails, the measurement systems, and the philosophical frameworks that ensure AI development benefits humanity. The +20-35% Agentic AI Premium applies because safety and alignment for agentic systems (tool use, multi-step autonomous reasoning, real-world action) represents the hardest unsolved problem in AI safety — and the most valuable expertise in the industry.

**Level Mapping:** Anthropic Constitutional AI Research Engineer = Google L6+ Research Scientist = DeepMind Senior Research Scientist = OpenAI Research Engineer = Meta FAIR Research Scientist IC6+

**The Safety Premium — $300K+ Base Benchmark**
Anthropic pays the highest base salaries in the AI industry — the "Safety Premium." This isn't charity; it's strategic. Anthropic attracts mission-driven talent who want to build AI safely, and the high base ensures they don't lose candidates to OpenAI's equity-heavy offers. Use Anthropic's base as your benchmark: "Anthropic is offering me $295K-$370K base for comparable work. Your base needs to match or the equity must compensate for the gap." For Constitutional AI Research Engineers, the Safety Premium is at its absolute peak — this role commands the highest base salaries at Anthropic because it requires the rarest combination of skills in the industry: production ML engineering, deep AI safety research knowledge, and the ability to translate alignment theory into working systems. The base range starts near $300K and extends to $370K, making this one of the highest-base IC roles in all of technology. The Safety Premium also provides financial stability during the pre-IPO period — you're not dependent on a liquidity event to earn market-rate compensation. When comparing offers, Anthropic's $295K-$370K base for this signature role is the floor, not the ceiling. No other company pays this much guaranteed cash for alignment and safety work — because no other company takes alignment this seriously.

**Global Levers**
1. **Signature Role Scarcity Premium:** "There are perhaps 200 people in the world who can do this job — engineers with production ML skills, deep understanding of RLHF and Constitutional AI, interpretability research experience, and the ability to build alignment systems at scale. That extreme scarcity means the compensation must reflect a market of near-zero supply. I'm looking for $365K base and $850K equity/4yr."
2. **Agentic AI Safety Premium (+20-35%):** "Agentic AI safety — ensuring that Claude can use tools, reason over multiple steps, and take real-world actions safely — is the hardest unsolved problem in alignment. I have direct research experience in [agentic safety / tool-use alignment / multi-step reasoning safety evaluation], and that expertise commands a 20-35% premium. My base ask of $365K reflects the agentic safety market rate."
3. **Competing Offers from AI Safety Labs:** "OpenAI is offering me $320K base / $1.1M equity for alignment research engineering. Google DeepMind is at $340K base / $700K RSU for interpretability research. Anthropic is where I want to be — Constitutional AI is the most rigorous alignment framework in the industry — but I need $365K base and $850K equity/4yr to match the total comp. This is the role I was born to do; help me do it at Anthropic."
4. **Existential Impact Framing:** "This role isn't about shipping features — it's about ensuring that the most powerful AI systems ever built remain safe, honest, and beneficial. The work I'll do here directly determines whether AI development goes well for humanity. That's not hyperbole; it's Anthropic's founding thesis. I'd like the equity to reflect the existential importance of this work — $860K/4yr with milestone-based refreshes tied to safety research publications and Constitutional AI improvements."
5. **Pre-IPO Equity Maximization for Core Research:** "Constitutional AI is Anthropic's core differentiator — it's why enterprises trust Claude, why regulators engage with Anthropic, and why the company is valued at $60B+. As someone building that differentiator, I should be among the most meaningfully invested employees. I'd like front-loaded vesting (40% year one) and a commitment to equity refreshes that keep my total grant competitive through IPO."

> **Negotiate Up Strategy:** "I've dedicated my career to the intersection of machine learning engineering and AI safety research — Constitutional AI, RLHF, interpretability, alignment evaluation. This is the work I believe matters most for humanity's future, and Anthropic is the only company doing it with the rigor and commitment it demands. I'm currently holding an OpenAI offer at $320K base / $1.1M equity for alignment research, and a Google DeepMind offer at $340K base / $700K RSU for interpretability. Both are strong. But Anthropic's Constitutional AI framework is the most complete alignment approach in the industry, and building it is the work I want to do. To make the move, I need $365K base, $850K equity/4yr with 40% year-one vesting, and a $100K signing bonus. The Agentic AI Safety Premium applies — alignment for agentic systems is the frontier of safety research, and my expertise in [tool-use safety / multi-step reasoning alignment / autonomous agent evaluation] is in extraordinary demand. At $365K base, I sign immediately and dedicate the next chapter of my career to making Claude the safest and most beneficial AI system in the world. My floor is $340K base — below that, the DeepMind liquid equity and research resources become the rational choice, even though my conviction is with Anthropic. This is the most important work in AI. Let's get the compensation right so I can start doing it."

**Evidence & Sources**
- Levels.fyi Anthropic Research Engineer and ML Engineer compensation data (2025-2026)
- Blind verified Anthropic alignment research and safety engineering offer threads
- AI Safety Talent Market Report — Constitutional AI and alignment research premium analysis (2025-2026)
- Anthropic careers page role descriptions and published research on Constitutional AI methodology
