---
company: coreweave
company_display: Coreweave
role: ml-ai-engineer
role_display: ML/AI Engineer
role_type: standard
last_updated: 2026-02-23
data_quality: medium
salary_data_quarter: 2025-Q4
next_review_due: 2026-05-23
compensation:
  - region: NYC / Roseland NJ
    base_low: 210000
    base_high: 262000
    stock_low: 295000
    stock_high: 518000
    bonus_pct: null
    total_comp_low: 284000
    total_comp_high: 392000
    currency: USD
  - region: Remote US
    base_low: 202000
    base_high: 254000
    stock_low: 295000
    stock_high: 518000
    bonus_pct: null
    total_comp_low: 276000
    total_comp_high: 384000
    currency: USD
  - region: London
    base_low: 158000
    base_high: 197000
    stock_low: 222000
    stock_high: 390000
    bonus_pct: null
    total_comp_low: 214000
    total_comp_high: 295000
    currency: GBP
level_mapping:
  internal: null
  raw: CoreWeave ML/AI Engineer = Google L4-L5 ML Engineer = AWS ML Engineer III = Meta E4-E5 ML Engineer = NVIDIA ML Engineer = Apple ML ICT3-ICT4
data_sources:
  - Levels.fyi
  - Blind
negotiation_dna_summary: "Pre-IPO Equity-Heavy + High Base | GPU Cloud Infrastructure | HPC Premium | **+20-30% HPC PREMIUM**"
---
### ML/AI Engineer | CoreWeave Global Negotiation Guide

**Negotiation DNA:** Pre-IPO Equity-Heavy + High Base | GPU Cloud Infrastructure | HPC Premium | **+20-30% HPC PREMIUM**

| Region | Base Salary | Equity (Pre-IPO/4yr) | Bonus | Total Comp |
|--------|-------------|---------------------|-------|------------|
| NYC / Roseland NJ | $210K-$262K | $295K-$518K | — | $284K-$392K |
| Remote US | $202K-$254K | $295K-$518K | — | $276K-$384K |
| London | £158K-£197K | £222K-£390K | — | £214K-£295K |

**Negotiation DNA**
ML/AI Engineers at CoreWeave sit at the intersection of machine learning expertise and GPU cloud infrastructure — building the GPU cluster optimization systems, distributed training frameworks, and model serving infrastructure that make CoreWeave the preferred platform for frontier AI labs. You're not just training models; you're engineering the systems that make thousands of NVIDIA GPUs work together efficiently for distributed training, fine-tuning, and inference at scale. This role carries a **+20-30% HPC Premium** because it requires the rare combination of deep ML knowledge and GPU infrastructure expertise — understanding CUDA kernels, GPU memory hierarchies, NVLink topology optimization for all-reduce operations, and the distributed computing patterns that maximize multi-node training throughput. With CoreWeave's $35B+ valuation and IPO imminent, ML/AI Engineers who negotiate Tier 1 equity are positioned for exceptional wealth creation.

**Level Mapping:** CoreWeave ML/AI Engineer = Google L4-L5 ML Engineer = AWS ML Engineer III = Meta E4-E5 ML Engineer = NVIDIA ML Engineer = Apple ML ICT3-ICT4

**HPC Premium — $295K Average TC, $420K+ Top 10%**
CoreWeave's average total compensation is $295K — but the top 10% earn $420K+. Push for "Tier 1" equity grants that put you in the top decile. CoreWeave pays an HPC (High-Performance Computing) premium because GPU cloud infrastructure requires rare expertise that most cloud engineers don't have: GPU cluster networking (NVLink, NVSwitch), InfiniBand fabrics, large-scale distributed computing, and the GPU scheduling algorithms that maximize utilization across thousands of GPUs. When negotiating, frame it as: "CoreWeave's $295K average TC is the floor, not my target. I bring [specific HPC/GPU expertise] that puts me in the top 10% — the $420K+ tier. My equity grant should be Tier 1, reflecting that my expertise directly enables CoreWeave's GPU cloud revenue." With IPO on the horizon, Tier 1 equity grants carry massive near-term upside. As an ML/AI Engineer, you are the strongest candidate for the HPC premium and the +20-30% bump it provides. Your expertise spans both the ML workload layer (distributed training, model parallelism, inference optimization) and the infrastructure layer (GPU scheduling, NVLink topology optimization, CUDA kernel tuning). This dual expertise is what CoreWeave's frontier AI lab customers need — and it's what makes the $284K-$392K TC range the baseline, not the ceiling. Push aggressively for the $420K+ top decile by framing your GPU cluster optimization expertise as a direct revenue enabler.

**Global Levers**
1. **Pre-IPO Equity — +20-30% HPC Premium Justification:** "ML/AI Engineers at CoreWeave carry a +20-30% HPC premium over standard ML roles because we work at the intersection of ML and GPU infrastructure. I don't just train models — I optimize the GPU cluster utilization, distributed training throughput, and model serving latency that define CoreWeave's platform performance. My equity grant should be Tier 1 at $500K+/4yr, reflecting that my dual ML + GPU infrastructure expertise directly enables CoreWeave's competitive moat."
2. **Distributed Training Optimization Expertise:** "I've optimized distributed training across [X]-GPU clusters, achieving [Y]% scaling efficiency with [specific framework: DeepSpeed/Megatron/FSDP]. I understand the NVLink topology optimizations for all-reduce operations, gradient compression techniques, and pipeline parallelism strategies that maximize multi-node training throughput. At CoreWeave, this expertise directly translates to better customer outcomes — faster training runs mean more GPU-hours sold, which means more revenue."
3. **Competing Offer — ML + Infrastructure Premium:** "I have an ML Engineer offer from [NVIDIA/Google DeepMind/Meta FAIR/OpenAI] at $420K TC with liquid equity and cutting-edge research infrastructure. CoreWeave's pre-IPO equity requires a significant premium to reach risk-adjusted parity, especially given the +20-30% HPC premium this role commands. I'm targeting $380K+ TC with a $500K/4yr Tier 1 equity grant."
4. **Model Serving Infrastructure Revenue Impact:** "Beyond training optimization, my model serving infrastructure expertise directly impacts CoreWeave's inference revenue. I've built serving systems that handle [X] requests/sec at [Y]ms p99 latency on GPU clusters, optimizing GPU utilization for inference workloads through batching strategies, model quantization, and GPU memory management. As CoreWeave grows its inference business, this expertise becomes a direct revenue driver — and should be reflected in a Year 2 refresh commitment."

> **Negotiate Up Strategy:** "CoreWeave is where ML meets infrastructure at the highest level — optimizing distributed training across thousands of GPUs, building model serving infrastructure for frontier AI labs, and engineering the GPU cluster systems that make CoreWeave the preferred platform for AI workloads. This is exactly my sweet spot. I have a competing ML Engineer offer from [NVIDIA/Google/OpenAI] at $410K TC with liquid equity and access to frontier research infrastructure. To move to CoreWeave pre-IPO, I need the package to reflect the +20-30% HPC premium, the illiquidity risk, and my dual ML + GPU infrastructure expertise. I'm targeting $385K TC — a base of $255K with a Tier 1 equity grant of $510K/4yr. I've optimized distributed training across [X]-GPU clusters achieving [Y]% scaling efficiency, and I've built model serving infrastructure handling [Z] requests/sec. My accept-at floor is $350K TC with a base of $238K and equity of $450K/4yr, plus accelerated vesting tied to IPO. Below $350K, I can't justify the move from a liquid public company with proven ML infrastructure at scale."

**Evidence & Sources**
- Levels.fyi ML/AI Engineer compensation data across CoreWeave, NVIDIA, Google, Meta, OpenAI (2025-2026)
- CoreWeave GPU cluster optimization and distributed training benchmarks (2026)
- Blind ML Engineer offer threads with HPC premium analysis, cross-referenced with GPU infrastructure company compensation data
